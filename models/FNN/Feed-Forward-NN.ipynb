{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda2/envs/nlp/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"/root/Objects/Fulldata_wY_500\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda2/envs/nlp/lib/python2.7/site-packages/pandas/core/generic.py:3295: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "train_data = data.loc[data.Date < '2009-01-01', :]\n",
    "dev_data = data.loc[(data.Date >= '2009-01-01') & (data.Date <= '2010-12-31'), :]\n",
    "test_data = data.loc[data.Date >= '2011-01-01', :]\n",
    "\n",
    "train_label = train_data.label\n",
    "dev_label = dev_data.label\n",
    "test_label = test_data.label\n",
    "\n",
    "#train_text_features.shape\n",
    "train_data['Surprise'].fillna(0, inplace=True)\n",
    "dev_data['Surprise'].fillna(0, inplace=True)\n",
    "#train_data.fillna(0, inplace=True)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "train_text_features = vectorizer.fit_transform(train_data.bow)\n",
    "\n",
    "dev_text_features = vectorizer.transform(dev_data.bow)\n",
    "\n",
    "\n",
    "################Combine Text Feature with Earning Surprise ################################\n",
    "all_data_train = np.append(train_text_features.toarray(),\\\n",
    "                           np.array(train_data.Surprise).reshape(len(train_data.Surprise),1), 1)\n",
    "all_data_dev = np.append(dev_text_features.toarray(),\\\n",
    "                         np.array(dev_data.Surprise).reshape(len(dev_data.Surprise),1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sample smaller subset of  dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#randomly drawn negative cases from negative case dataset\n",
    "randomw_draw = random.sample(range(0, dev_data.shape[0]), 5000) #11000\n",
    "all_data2 = dev_data.iloc[randomw_draw,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Y label  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_label(data_in):\n",
    "#Generate new labels\n",
    "  label_y = []\n",
    "  for sign in data_in:\n",
    "    if sign == 'DOWN':\n",
    "      label_y.append([0, 0, 1])\n",
    "    elif sign == 'STAY':\n",
    "      label_y.append([0, 1, 0])\n",
    "    else:\n",
    "      label_y.append([1, 0, 0])\n",
    "  \n",
    "  label_y = np.asarray(label_y)\n",
    "  return(label_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = create_label(train_data.label)\n",
    "y_dev = create_label(dev_data.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TensforFlow Code for  Feed Forward neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 100, train accuracy = 45.86%, test accuracy = 46.62%\n",
      "Epoch = 200, train accuracy = 45.92%, test accuracy = 47.33%\n",
      "Epoch = 300, train accuracy = 48.04%, test accuracy = 47.83%\n",
      "Epoch = 400, train accuracy = 48.38%, test accuracy = 48.02%\n",
      "Epoch = 500, train accuracy = 48.70%, test accuracy = 48.13%\n",
      "Epoch = 600, train accuracy = 49.16%, test accuracy = 48.40%\n",
      "Epoch = 700, train accuracy = 50.52%, test accuracy = 48.61%\n",
      "Epoch = 800, train accuracy = 50.46%, test accuracy = 48.69%\n",
      "Epoch = 900, train accuracy = 50.08%, test accuracy = 48.91%\n",
      "Epoch = 1000, train accuracy = 51.78%, test accuracy = 48.95%\n"
     ]
    }
   ],
   "source": [
    "def init_weights(shape):\n",
    "    \"\"\"\n",
    "    Initialized Weights\n",
    "    \"\"\"\n",
    "    weights = tf.random_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(weights)\n",
    "\n",
    "def forwardprop(X, w_1, w_2):\n",
    "    \"\"\"\n",
    "    Forward Propagation.\n",
    "    \"\"\"\n",
    "    h    = tf.nn.sigmoid(tf.matmul(X, w_1))  # The \\sigma function\n",
    "    yhat = tf.matmul(h, w_2)  # The \\varphi function\n",
    "    return yhat\n",
    "\n",
    "\n",
    "# Layer's sizes\n",
    "x_size = all_data_train.shape[1]   # Number of inputs\n",
    "h_size = 200   #256                # Hidden nodes\n",
    "y_size = 3   # Number of class\n",
    "\n",
    "# Input/Output\n",
    "X = tf.placeholder(\"float\", shape=[None, x_size])\n",
    "y = tf.placeholder(\"float\", shape=[None, y_size])\n",
    "\n",
    "# Weight initializations\n",
    "w1 = init_weights((x_size, h_size))\n",
    "w2 = init_weights((h_size, y_size))\n",
    "\n",
    "# Forward propagation\n",
    "y_hat    = forwardprop(X, w1, w2)\n",
    "predict = tf.argmax(yhat, dimension=1)\n",
    "\n",
    "# Backward propagation\n",
    "cost    = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_hat, y))\n",
    "updates = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "# Run SGD\n",
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    #Randomly Draw a sample of data for training each epoch\n",
    "    randomw_draw = random.sample(range(0, all_data_train.shape[0]), 5000) #11000\n",
    "    all_data2 = all_data_train[randomw_draw,:]\n",
    "    y_train2 = y_train[randomw_draw,:]\n",
    "    # Train with each example\n",
    "   \n",
    "    sess.run([updates], feed_dict={X: all_data2, y: y_train2})\n",
    "\n",
    "    train_accuracy = np.mean(np.argmax(y_train2, axis=1) ==\n",
    "                                 sess.run(predict, feed_dict={X: all_data2, y: y_train2}))\n",
    "    test_accuracy  = np.mean(np.argmax(y_dev, axis=1) ==\n",
    "                                 sess.run(predict, feed_dict={X: all_data_dev, y: y_dev}))\n",
    "    #train_accuracy = 0\n",
    "    if (epoch +1) % 100 == 0:\n",
    "      print(\"Epoch = %d, train accuracy = %.2f%%, test accuracy = %.2f%%\" % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 50, train accuracy = 0.00%, test accuracy = 46.36%\n",
      "Epoch = 100, train accuracy = 0.00%, test accuracy = 47.09%\n",
      "Epoch = 150, train accuracy = 0.00%, test accuracy = 47.40%\n",
      "Epoch = 200, train accuracy = 0.00%, test accuracy = 47.82%\n",
      "Epoch = 250, train accuracy = 0.00%, test accuracy = 47.95%\n",
      "Epoch = 300, train accuracy = 0.00%, test accuracy = 48.15%\n",
      "Epoch = 350, train accuracy = 0.00%, test accuracy = 48.15%\n",
      "Epoch = 400, train accuracy = 0.00%, test accuracy = 48.22%\n",
      "Epoch = 450, train accuracy = 0.00%, test accuracy = 48.26%\n",
      "Epoch = 500, train accuracy = 0.00%, test accuracy = 48.42%\n",
      "Epoch = 550, train accuracy = 0.00%, test accuracy = 48.50%\n",
      "Epoch = 600, train accuracy = 0.00%, test accuracy = 48.72%\n",
      "Epoch = 650, train accuracy = 0.00%, test accuracy = 48.90%\n",
      "Epoch = 700, train accuracy = 0.00%, test accuracy = 48.94%\n",
      "Epoch = 750, train accuracy = 0.00%, test accuracy = 48.97%\n",
      "Epoch = 800, train accuracy = 0.00%, test accuracy = 49.19%\n",
      "Epoch = 850, train accuracy = 0.00%, test accuracy = 49.20%\n",
      "Epoch = 900, train accuracy = 0.00%, test accuracy = 49.14%\n",
      "Epoch = 950, train accuracy = 0.00%, test accuracy = 49.25%\n",
      "Epoch = 1000, train accuracy = 0.00%, test accuracy = 49.32%\n",
      "Epoch = 1050, train accuracy = 0.00%, test accuracy = 49.32%\n",
      "Epoch = 1100, train accuracy = 0.00%, test accuracy = 49.33%\n",
      "Epoch = 1150, train accuracy = 0.00%, test accuracy = 49.33%\n",
      "Epoch = 1200, train accuracy = 0.00%, test accuracy = 49.44%\n",
      "Epoch = 1250, train accuracy = 0.00%, test accuracy = 49.47%\n",
      "Epoch = 1300, train accuracy = 0.00%, test accuracy = 49.43%\n",
      "Epoch = 1350, train accuracy = 0.00%, test accuracy = 49.63%\n"
     ]
    }
   ],
   "source": [
    "#RANDOM_SEED = 42\n",
    "#tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "def init_weights(shape):\n",
    "    \"\"\" Weight initialization \"\"\"\n",
    "    weights = tf.random_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(weights)\n",
    "\n",
    "def forwardprop(X, w_1, w_2):\n",
    "    \"\"\"\n",
    "    Forward-propagation.\n",
    "    IMPORTANT: yhat is not softmax since TensorFlow's softmax_cross_entropy_with_logits() does that internally.\n",
    "    \"\"\"\n",
    "    h    = tf.nn.sigmoid(tf.matmul(X, w_1))  # The \\sigma function\n",
    "    yhat = tf.matmul(h, w_2)  # The \\varphi function\n",
    "    return yhat\n",
    "\n",
    "\n",
    "# Layer's sizes\n",
    "x_size = all_data_train.shape[1]   # Number of input nodes: 4 features and 1 bias\n",
    "h_size = 256                # Number of hidden nodes\n",
    "y_size = 3   # Number of outcomes (3 iris flowers)\n",
    "\n",
    "# Symbols\n",
    "X = tf.placeholder(\"float\", shape=[None, x_size])\n",
    "y = tf.placeholder(\"float\", shape=[None, y_size])\n",
    "\n",
    "# Weight initializations\n",
    "w1 = init_weights((x_size, h_size))\n",
    "w2 = init_weights((h_size, y_size))\n",
    "\n",
    "# Forward propagation\n",
    "yhat    = forwardprop(X, w1, w2)\n",
    "predict = tf.argmax(yhat, dimension=1)\n",
    "\n",
    "# Backward propagation\n",
    "cost    = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(yhat, y))\n",
    "updates = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "# Run SGD\n",
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(1500):\n",
    "  \n",
    "    randomw_draw = random.sample(range(0, all_data_train.shape[0]), 5000) #11000\n",
    "    all_data2 = all_data_train[randomw_draw,:]\n",
    "    y_train2 = y_train[randomw_draw,:]\n",
    "    # Train with each example\n",
    "   \n",
    "    sess.run([updates], feed_dict={X: all_data2, y: y_train2})\n",
    "\n",
    "    #train_accuracy = np.mean(np.argmax(y_train2, axis=1) ==\n",
    "    #                             sess.run(predict, feed_dict={X: all_data2, y: y_train2}))\n",
    "    test_accuracy  = np.mean(np.argmax(y_dev, axis=1) ==\n",
    "                                 sess.run(predict, feed_dict={X: all_data_dev, y: y_dev}))\n",
    "    \n",
    "    #Train Accuracy set to zero to speed up the process\n",
    "    train_accuracy = 0\n",
    "    if (epoch +1) % 50 == 0:\n",
    "      print(\"Epoch = %d, train accuracy = %.2f%%, test accuracy = %.2f%%\" % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy))\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

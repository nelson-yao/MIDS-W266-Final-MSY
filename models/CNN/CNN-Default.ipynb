{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import data_processor\n",
    "import cnn_net\n",
    "import datetime\n",
    "from cnn_net import CNN\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import *\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from tensorflow.contrib import learn\n",
    "import data_processor as dp\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data from a previously saved pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data = pickle.load(open(\"/Users/Maximus/downloads/Fulldata_wY_fullText\", \"rb\"))\n",
    "data = pickle.load(open(\"/root/Objects/Fulldata_wY_500\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split data into train/dev/test sets\n",
    "train_data = data.loc[data.Date < '2009-01-01', :]\n",
    "dev_data = data.loc[(data.Date >= '2009-01-01') & (data.Date <= '2010-12-31'), :]\n",
    "test_data = data.loc[data.Date >= '2011-01-01', :]\n",
    "\n",
    "train_label = train_data.label\n",
    "dev_label = dev_data.label\n",
    "test_label = test_data.label\n",
    "\n",
    "#Replace na with 0\n",
    "train_data['Surprise'].fillna(0, inplace=True)\n",
    "dev_data['Surprise'].fillna(0, inplace=True)\n",
    "#train_data.fillna(0, inplace=True)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Re-format Label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_label(data_in):\n",
    "#Generate new labels\n",
    "  label_y = []\n",
    "  for sign in data_in:\n",
    "    if sign == 'DOWN':\n",
    "      label_y.append([1, 0, 0])\n",
    "    elif sign == 'STAY':\n",
    "      label_y.append([0, 1, 0])\n",
    "    else:\n",
    "      label_y.append([0, 0, 1])\n",
    "  \n",
    "  label_y = np.asarray(label_y)\n",
    "  return(label_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This random draw is created to randomly select a smaller set of data than the entire dataset.\n",
    "#Due to resource contraint, this is a more effcient way to train our model.\n",
    "\n",
    "import random\n",
    "random.seed(9)\n",
    "\n",
    "#randomly drawn a smaller subset from training and dev set due to the large sample size\n",
    "randomw_draw = random.sample(range(0, dev_data.shape[0]), 5000) \n",
    "dev_data2 = dev_data.iloc[randomw_draw,:]\n",
    "\n",
    "randomw_draw = random.sample(range(0, train_data.shape[0]), 12000) #17000 #Total\n",
    "train_data2 = train_data.iloc[randomw_draw,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding  Construction\n",
    "##### Select the top doc_n words from input paragrahp (excluding stopwords and symbols).  Create a vocabulary, then convert into numerical indices for word embedding in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 Done\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "##Number of words per document to use:\n",
    "doc_n = 3000\n",
    "\n",
    "#load_features has a parser that parses the financial text so the resulting paragraph is closer to natural language\n",
    "x_text = dp.load_features(train_data2.text, doc_n)\n",
    "\n",
    "# Build Vocabulary - tranform training input words with vocab_processor\n",
    "max_document_length = doc_n   #max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "print \"Part 1 Done\"\n",
    "del x_text\n",
    "\n",
    "#Build Vocabulary for the development set - tranform develop input words with vocab_processor\n",
    "x_text = dp.load_features(dev_data2.text, doc_n)\n",
    "x_dev = np.array(list(vocab_processor.transform(x_text)))\n",
    "\n",
    "#Create the numerical feature of earning surprise for our models\n",
    "surprise_train, surprise_dev = np.array(train_data2.Surprise), np.array(dev_data2.Surprise)\n",
    "surprise_train = np.reshape(surprise_train, (surprise_train.shape[0], 1))\n",
    "surprise_dev = np.reshape(surprise_dev, (surprise_dev.shape[0], 1))\n",
    "\n",
    "y_train = create_label(train_data2.label)\n",
    "y_dev = create_label(dev_data2.label)\n",
    "print \"Finished\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save result for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"x_train\", x_train, allow_pickle=True)\n",
    "np.save(\"x_dev\", x_dev, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Delete Data to save memory\n",
    "del train_data\n",
    "del train_data2\n",
    "del dev_data\n",
    "del dev_data2\n",
    "del x_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cnn_net' from 'cnn_net.pyc'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reload Module\n",
    "reload(data_processor)\n",
    "reload(cnn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We weren't able to save all the results from models with different hyperparameters.  However, below is the sample code we use to run Convolution Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample CNN Implementations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T10:57:47.849883: step 100, loss 150.289, acc 0.559322\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T11:22:23.189610: step 200, loss 3.755, acc 0.333333\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-18T11:24:52.496297: step 200, loss 3.63621, acc 0.3934\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T11:49:43.539803: step 300, loss 1.09656, acc 0.451977\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T12:14:10.384034: step 400, loss 1.09018, acc 0.38983\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-18T12:16:39.098753: step 400, loss 1.09448, acc 0.3928\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T12:41:17.630859: step 500, loss 1.07667, acc 0.451977\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T13:05:57.252270: step 600, loss 1.09293, acc 0.423729\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-18T13:08:24.188452: step 600, loss 1.09319, acc 0.3862\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T13:33:05.291789: step 700, loss 1.0989, acc 0.355932\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T13:57:43.233941: step 800, loss 1.08071, acc 0.38983\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-18T14:00:14.117969: step 800, loss 1.10521, acc 0.4038\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T14:24:50.147728: step 900, loss 1.09573, acc 0.429379\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T14:49:34.780771: step 1000, loss 1.10103, acc 0.361582\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-18T14:52:00.792826: step 1000, loss 1.10078, acc 0.3676\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T15:16:30.938802: step 1100, loss 1.09985, acc 0.423729\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T15:41:16.070732: step 1200, loss 1.09598, acc 0.412429\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-18T15:43:45.124520: step 1200, loss 1.11169, acc 0.371\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T16:08:28.523636: step 1300, loss 1.08613, acc 0.39548\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T16:33:03.214862: step 1400, loss 1.05769, acc 0.497175\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-18T16:35:31.371902: step 1400, loss 1.10037, acc 0.416\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T17:00:12.385568: step 1500, loss 1.08228, acc 0.463277\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T17:24:57.312114: step 1600, loss 1.08635, acc 0.446328\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-18T17:27:27.328253: step 1600, loss 1.09382, acc 0.4124\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T17:51:59.706846: step 1700, loss 1.09642, acc 0.397163\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T18:16:39.423600: step 1800, loss 1.08363, acc 0.412429\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-18T18:19:07.566746: step 1800, loss 1.09782, acc 0.4058\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T18:43:49.872238: step 1900, loss 1.09167, acc 0.384181\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T19:08:29.086852: step 2000, loss 1.08317, acc 0.412429\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-18T19:10:55.614527: step 2000, loss 1.10297, acc 0.3748\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T19:34:02.160764: step 2100, loss 1.08821, acc 0.38983\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T19:56:59.398625: step 2200, loss 1.11415, acc 0.429379\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-18T19:59:15.138856: step 2200, loss 1.10469, acc 0.3992\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T20:22:02.768646: step 2300, loss 1.11233, acc 0.350282\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T20:44:59.772249: step 2400, loss 1.09254, acc 0.367232\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-18T20:47:11.876499: step 2400, loss 1.09585, acc 0.3958\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T21:10:06.154809: step 2500, loss 1.09577, acc 0.361582\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T21:33:00.404222: step 2600, loss 1.08726, acc 0.38983\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-18T21:35:14.292405: step 2600, loss 1.097, acc 0.3888\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T21:58:13.166875: step 2700, loss 1.08928, acc 0.38983\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T22:21:14.278731: step 2800, loss 1.09745, acc 0.338983\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-18T22:23:29.812202: step 2800, loss 1.10835, acc 0.487\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-18T22:46:22.078159: step 2900, loss 1.09657, acc 0.38983\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-76af43958fa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-76af43958fa6>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch, s_batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             _, step,loss, accuracy = sess.run(\n\u001b[1;32m     54\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###Example 1 with 3000 words, large batch size 555\n",
    "\n",
    "# Hyperparameters\n",
    "######################################################\n",
    "M = 124  #Embedding Size\n",
    "FS = [1,2,3] #Filter Sizes\n",
    "Num_F = 124 #Number of filters per filte rsize\n",
    "dropout_keep_prob = 1\n",
    "L = 1000\n",
    "batch_size = 177\n",
    "num_epoch = 100\n",
    "#####################################################\n",
    "\n",
    "#Create Dev set\n",
    "dev = dp.one_iter(list(zip(x_dev, y_dev, surprise_dev))) \n",
    "for i in dev:\n",
    "  x_dbatch, y_dbatch, s_dbatch = zip(*i)\n",
    "         \n",
    "\n",
    "with tf.Graph().as_default():\n",
    "   \n",
    "    sess = tf.Session(config=tf.ConfigProto(device_filters=\"/cpu:0\"))\n",
    "    with sess.as_default():\n",
    "        cnn = CNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size = M,\n",
    "            filter_sizes = FS,\n",
    "            num_filters = Num_F,\n",
    "            l2_reg_lambda = L)\n",
    "\n",
    "        #Setup Training \n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        \n",
    "        # Initialize \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch, s_batch):\n",
    "            \"\"\"\n",
    "            One Training Step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.input_suprise: s_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step,loss, accuracy = sess.run(\n",
    "                [train_op, global_step, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if step % 100 == 0:\n",
    "              print(\"\\nTrain Checkpoint:\")\n",
    "              print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            \n",
    "        \n",
    "        def dev_step(x_batch, y_batch, s_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.input_suprise: s_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            step, loss, accuracy, predictions = sess.run(\n",
    "                [global_step, cnn.loss, cnn.accuracy, cnn.predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "           \n",
    "          \n",
    "        # Generate training batches\n",
    "        batches = data_processor.batch_iter(\n",
    "            list(zip(x_train, y_train, surprise_train)), batch_size, num_epoch)\n",
    "        \n",
    "        for batch in batches:\n",
    "            x_batch, y_batch, s_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch, s_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "            if current_step % 200 == 0:\n",
    "                print(\"\\nDev Set Evaluation:\")\n",
    "                dev_step(x_dbatch, y_dbatch, s_dbatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Smaller batch (74) with 2000 words per record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T10:59:38.691790: step 100, loss 1741.64, acc 0.310811\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T11:10:55.210979: step 200, loss 34.1061, acc 0.310811\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T11:13:23.590202: step 200, loss 32.8919, acc 0.2546\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T11:24:54.318806: step 300, loss 1.25294, acc 0.391892\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T11:36:09.566092: step 400, loss 1.09923, acc 0.364865\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T11:38:40.972046: step 400, loss 1.09446, acc 0.4058\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T11:50:02.222860: step 500, loss 1.09985, acc 0.351351\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T12:01:25.880083: step 600, loss 1.10384, acc 0.391892\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T12:04:02.816054: step 600, loss 1.09474, acc 0.3996\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T12:15:16.512116: step 700, loss 1.0982, acc 0.405405\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T12:26:42.247399: step 800, loss 1.10486, acc 0.324324\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T12:29:10.937594: step 800, loss 1.0948, acc 0.4078\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T12:40:32.994591: step 900, loss 1.09857, acc 0.351351\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T12:51:50.776070: step 1000, loss 1.09476, acc 0.527027\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T12:54:15.308255: step 1000, loss 1.09562, acc 0.3978\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T13:05:39.002605: step 1100, loss 1.09842, acc 0.364865\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T13:16:59.982739: step 1200, loss 1.09552, acc 0.418919\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T13:19:31.108705: step 1200, loss 1.09554, acc 0.3954\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T13:31:03.455232: step 1300, loss 1.10341, acc 0.364865\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T13:42:21.330738: step 1400, loss 1.09953, acc 0.337838\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T13:44:55.135237: step 1400, loss 1.0997, acc 0.371\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T13:56:16.188223: step 1500, loss 1.09925, acc 0.378378\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T14:07:37.398880: step 1600, loss 1.09262, acc 0.418919\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T14:10:10.295178: step 1600, loss 1.09507, acc 0.4204\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T14:21:32.068116: step 1700, loss 1.10208, acc 0.324324\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T14:32:55.480388: step 1800, loss 1.10168, acc 0.337838\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T14:35:22.278270: step 1800, loss 1.10615, acc 0.3076\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T14:46:48.631959: step 1900, loss 1.1035, acc 0.310811\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T14:58:09.231361: step 2000, loss 1.09855, acc 0.297297\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T15:00:41.938686: step 2000, loss 1.09601, acc 0.3732\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T15:12:08.670854: step 2100, loss 1.09238, acc 0.432432\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T15:23:29.042169: step 2200, loss 1.1014, acc 0.337838\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T15:25:58.508304: step 2200, loss 1.09524, acc 0.4406\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T15:37:20.782709: step 2300, loss 1.12331, acc 0.418919\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T15:48:42.936183: step 2400, loss 1.09491, acc 0.445946\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T15:51:15.065441: step 2400, loss 1.09482, acc 0.4414\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T16:02:37.942846: step 2500, loss 1.10152, acc 0.445946\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T16:14:05.631908: step 2600, loss 1.0934, acc 0.364865\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T16:16:37.959631: step 2600, loss 1.09578, acc 0.4108\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T16:28:00.524548: step 2700, loss 1.0965, acc 0.445946\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T16:39:18.985463: step 2800, loss 1.09742, acc 0.486486\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T16:41:47.481818: step 2800, loss 1.09818, acc 0.3792\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T16:53:14.188201: step 2900, loss 1.09563, acc 0.5\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T17:04:34.310585: step 3000, loss 1.10216, acc 0.351351\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T17:07:03.658224: step 3000, loss 1.10286, acc 0.3692\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T17:18:26.944256: step 3100, loss 1.09281, acc 0.364865\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T17:29:53.810719: step 3200, loss 1.10168, acc 0.324324\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T17:32:28.188048: step 3200, loss 1.09663, acc 0.402\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T17:43:44.770793: step 3300, loss 1.09105, acc 0.527027\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T17:55:03.918920: step 3400, loss 1.0972, acc 0.432432\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T17:57:34.803557: step 3400, loss 1.0953, acc 0.4018\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T18:09:01.550733: step 3500, loss 1.10183, acc 0.391892\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T18:20:19.610739: step 3600, loss 1.09973, acc 0.472973\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T18:22:41.941979: step 3600, loss 1.0981, acc 0.4006\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T18:34:11.261956: step 3700, loss 1.10515, acc 0.310811\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-17T18:45:21.202856: step 3800, loss 1.09737, acc 0.486486\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-17T18:47:52.000098: step 3800, loss 1.10558, acc 0.3962\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-08012dc093b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-08012dc093b1>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch, s_batch)\u001b[0m\n\u001b[1;32m     83\u001b[0m             _, step,loss, accuracy = sess.run(\n\u001b[1;32m     84\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#########################\n",
    "##Start Training       ##\n",
    "#########################\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "######################################################\n",
    "M = 124  #Embedding Size\n",
    "FS = [1,2,3] #Filter Sizes\n",
    "Num_F = 124 #Number of filters per filte rsize\n",
    "dropout_keep_prob = 1\n",
    "L = 10000\n",
    "batch_size = 74\n",
    "num_epoch = 100\n",
    "#####################################################\n",
    "\n",
    "#Create Dev set\n",
    "dev = dp.one_iter(list(zip(x_dev, y_dev, surprise_dev))) \n",
    "for i in dev:\n",
    "  x_dbatch, y_dbatch, s_dbatch = zip(*i)\n",
    "         \n",
    "\n",
    "with tf.Graph().as_default():\n",
    "   \n",
    "    sess = tf.Session(config=tf.ConfigProto(device_filters=\"/cpu:0\"))\n",
    "    with sess.as_default():\n",
    "        cnn = CNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size = M,\n",
    "            filter_sizes = FS,\n",
    "            num_filters = Num_F,\n",
    "            l2_reg_lambda = L)\n",
    "\n",
    "        #Setup Training \n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Initialize \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch, s_batch):\n",
    "            \"\"\"\n",
    "            One Training Step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.input_suprise: s_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step,loss, accuracy = sess.run(\n",
    "                [train_op, global_step, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if step % 100 == 0:\n",
    "              print(\"\\nTrain Checkpoint:\")\n",
    "              print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            \n",
    "        \n",
    "        def dev_step(x_batch, y_batch, s_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.input_suprise: s_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            step, loss, accuracy = sess.run(\n",
    "                [global_step, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))     \n",
    "           \n",
    "          \n",
    "        # Generate batches\n",
    "        batches = data_processor.batch_iter(\n",
    "            list(zip(x_train, y_train, surprise_train)), batch_size, num_epoch)\n",
    "        \n",
    "        for batch in batches:\n",
    "            x_batch, y_batch, s_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch, s_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "            if current_step % 200 == 0:\n",
    "                print(\"\\nDev Set Evaluation:\")\n",
    "                dev_step(x_dbatch, y_dbatch, s_dbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Example 2 with 3000 words, large batch size 215\n",
    "\n",
    "# Hyperparameters\n",
    "######################################################\n",
    "M = 124  #Embedding Size\n",
    "FS = [2,3,4] #Filter Sizes\n",
    "Num_F = 124 #Number of filters per filte rsize\n",
    "dropout_keep_prob = 1\n",
    "L = 10000\n",
    "batch_size = 215\n",
    "num_epoch = 100\n",
    "#####################################################\n",
    "\n",
    "#Create Dev set\n",
    "dev = dp.one_iter(list(zip(x_dev, y_dev, surprise_dev))) \n",
    "for i in dev:\n",
    "  x_dbatch, y_dbatch, s_dbatch = zip(*i)\n",
    "         \n",
    "\n",
    "with tf.Graph().as_default():\n",
    "   \n",
    "    sess = tf.Session(config=tf.ConfigProto(device_filters=\"/cpu:0\"))\n",
    "    with sess.as_default():\n",
    "        cnn = CNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size = M,\n",
    "            filter_sizes = FS,\n",
    "            num_filters = Num_F,\n",
    "            l2_reg_lambda = L)\n",
    "\n",
    "        #Setup Training \n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        \n",
    "        # Initialize \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch, s_batch):\n",
    "            \"\"\"\n",
    "            One Training Step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.input_suprise: s_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step,loss, accuracy = sess.run(\n",
    "                [train_op, global_step, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if step % 100 == 0:\n",
    "              print(\"\\nTrain Checkpoint:\")\n",
    "              print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            \n",
    "        \n",
    "        def dev_step(x_batch, y_batch, s_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.input_suprise: s_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            step, loss, accuracy = sess.run(\n",
    "                [global_step, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))     \n",
    "           \n",
    "          \n",
    "        # Generate training batches\n",
    "        batches = data_processor.batch_iter(\n",
    "            list(zip(x_train, y_train, surprise_train)), batch_size, num_epoch)\n",
    "        \n",
    "        for batch in batches:\n",
    "            x_batch, y_batch, s_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch, s_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "            if current_step % 200 == 0:\n",
    "                print(\"\\nDev Set Evaluation:\")\n",
    "                dev_step(x_dbatch, y_dbatch, s_dbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Checkpoint:\n",
      "2016-12-14T07:37:48.960365: step 100, loss 1801.02, acc 0.506667\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-14T07:38:24.624259: step 100, loss 1741.53, acc 0.496\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-14T07:42:01.253003: step 200, loss 36.1494, acc 0.48\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-14T07:42:36.143203: step 200, loss 34.5943, acc 0.44\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-14T07:46:20.647712: step 300, loss 1.28033, acc 0.426667\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-14T07:46:58.458832: step 300, loss 1.25946, acc 0.411333\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-14T07:51:01.637704: step 400, loss 1.09757, acc 0.386667\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-14T07:51:39.850304: step 400, loss 1.0953, acc 0.402667\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-14T07:55:44.956532: step 500, loss 1.09913, acc 0.4\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-14T07:56:24.322188: step 500, loss 1.09484, acc 0.410333\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-14T08:00:27.721672: step 600, loss 1.09869, acc 0.386667\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-14T08:01:07.457427: step 600, loss 1.09655, acc 0.416333\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-14T08:05:13.036766: step 700, loss 1.09951, acc 0.36\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-14T08:05:51.716954: step 700, loss 1.09552, acc 0.407\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-14T08:09:55.531863: step 800, loss 1.09598, acc 0.426667\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-14T08:10:35.033416: step 800, loss 1.09583, acc 0.394333\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-14T08:14:38.961503: step 900, loss 1.09781, acc 0.44\n",
      "\n",
      "Dev Set Evaluation:\n",
      "2016-12-14T08:15:17.344455: step 900, loss 1.09503, acc 0.411667\n",
      "\n",
      "Train Checkpoint:\n",
      "2016-12-14T08:19:19.500277: step 1000, loss 1.09746, acc 0.426667\n",
      "\n",
      "Dev Set Evaluation:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6909940a6635>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDev Set Evaluation:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mdev_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurprise_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-6909940a6635>\u001b[0m in \u001b[0;36mdev_step\u001b[0;34m(x_batch, y_batch, s_batch, writer)\u001b[0m\n\u001b[1;32m     96\u001b[0m             step, loss, accuracy = sess.run(\n\u001b[1;32m     97\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, acc {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#####\n",
    "##Start Training\n",
    "#####\n",
    "\n",
    "# Hyperparameters\n",
    "######################################################\n",
    "M = 124  #Embedding Size\n",
    "FS = [1,2,3] #Filter Sizes\n",
    "Num_F = 124 #Number of filters per filte rsize\n",
    "dropout_keep_prob = 1\n",
    "L = 10000\n",
    "batch_size = 75\n",
    "num_epoch = 100\n",
    "#####################################################\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "   \n",
    "    sess = tf.Session(config=tf.ConfigProto(device_filters=\"/cpu:0\"))\n",
    "    with sess.as_default():\n",
    "        cnn = CNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size = M,\n",
    "            filter_sizes = FS,\n",
    "            num_filters = Num_F,\n",
    "            l2_reg_lambda = L)\n",
    "\n",
    "        #Setup Training \n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Initialize \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch, s_batch):\n",
    "            \"\"\"\n",
    "            One Training Step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.input_suprise: s_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step,loss, accuracy = sess.run(\n",
    "                [train_op, global_step, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if step % 100 == 0:\n",
    "              print(\"\\nTrain Checkpoint:\")\n",
    "              print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            \n",
    "        \n",
    "        def dev_step(x_batch, y_batch, s_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.input_suprise: s_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            step, loss, accuracy = sess.run(\n",
    "                [global_step, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))     \n",
    "           \n",
    "          \n",
    "        # Generate batches\n",
    "        batches = data_processor.batch_iter(\n",
    "            list(zip(x_train, y_train, surprise_train)), batch_size, num_epoch)\n",
    "        \n",
    "        for batch in batches:\n",
    "            x_batch, y_batch, s_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch, s_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % 100 == 0:\n",
    "                print(\"\\nDev Set Evaluation:\")\n",
    "                dev_step(x_dev, y_dev, surprise_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-13T06:51:34.165871: step 50, loss 80.0191, acc 0.4\n",
      "2016-12-13T06:53:39.802057: step 100, loss 17.1919, acc 0.346667\n",
      "\n",
      "Dev set Evaluation:\n",
      "2016-12-13T06:54:25.884365: step 100, loss 16.6229, acc 0.372\n",
      "2016-12-13T06:56:30.860248: step 150, loss 3.56904, acc 0.453333\n",
      "2016-12-13T06:58:36.826978: step 200, loss 1.34105, acc 0.453333\n",
      "\n",
      "Dev set Evaluation:\n",
      "2016-12-13T06:59:22.740437: step 200, loss 1.44876, acc 0.396333\n",
      "2016-12-13T07:01:28.282701: step 250, loss 1.10076, acc 0.4\n",
      "2016-12-13T07:03:33.206529: step 300, loss 1.04698, acc 0.466667\n",
      "\n",
      "Dev set Evaluation:\n",
      "2016-12-13T07:04:19.456236: step 300, loss 1.13621, acc 0.438333\n",
      "2016-12-13T07:06:25.403664: step 350, loss 1.09456, acc 0.32\n",
      "2016-12-13T07:08:30.895494: step 400, loss 1.04159, acc 0.493333\n",
      "\n",
      "Dev set Evaluation:\n",
      "2016-12-13T07:09:17.100089: step 400, loss 1.1571, acc 0.391333\n",
      "2016-12-13T07:11:21.145860: step 450, loss 1.08969, acc 0.36\n",
      "2016-12-13T07:13:27.814229: step 500, loss 1.10441, acc 0.36\n",
      "\n",
      "Dev set Evaluation:\n",
      "2016-12-13T07:14:13.965670: step 500, loss 1.18917, acc 0.373\n",
      "2016-12-13T07:16:19.529546: step 550, loss 1.02576, acc 0.506667\n",
      "2016-12-13T07:18:24.266508: step 600, loss 1.10288, acc 0.386667\n",
      "\n",
      "Dev set Evaluation:\n",
      "2016-12-13T07:19:10.338615: step 600, loss 1.14636, acc 0.473\n",
      "2016-12-13T07:21:16.526712: step 650, loss 1.15742, acc 0.32\n",
      "2016-12-13T07:23:22.453779: step 700, loss 1.0677, acc 0.44\n",
      "\n",
      "Dev set Evaluation:\n",
      "2016-12-13T07:24:08.315523: step 700, loss 1.16825, acc 0.399667\n",
      "2016-12-13T07:26:13.371135: step 750, loss 1.28958, acc 0.426667\n",
      "2016-12-13T07:28:19.170061: step 800, loss 1.05419, acc 0.506667\n",
      "\n",
      "Dev set Evaluation:\n",
      "2016-12-13T07:29:05.460292: step 800, loss 1.13688, acc 0.398\n",
      "2016-12-13T07:31:11.503682: step 850, loss 1.16944, acc 0.28\n",
      "2016-12-13T07:33:16.511805: step 900, loss 1.05816, acc 0.453333\n",
      "\n",
      "Dev set Evaluation:\n",
      "2016-12-13T07:34:02.842739: step 900, loss 1.14644, acc 0.394\n",
      "2016-12-13T07:36:09.345418: step 950, loss 1.11304, acc 0.426667\n",
      "2016-12-13T07:38:15.850263: step 1000, loss 1.08133, acc 0.453333\n",
      "\n",
      "Dev set Evaluation:\n",
      "2016-12-13T07:39:02.171253: step 1000, loss 1.11414, acc 0.465333\n",
      "2016-12-13T07:41:08.799820: step 1050, loss 1.08117, acc 0.453333\n",
      "2016-12-13T07:43:15.110251: step 1100, loss 1.0874, acc 0.386667\n",
      "\n",
      "Dev set Evaluation:\n",
      "2016-12-13T07:44:01.129010: step 1100, loss 1.11419, acc 0.410333\n",
      "2016-12-13T07:46:06.990641: step 1150, loss 1.08909, acc 0.36\n",
      "2016-12-13T07:48:11.524691: step 1200, loss 1.07874, acc 0.373333\n",
      "\n",
      "Dev set Evaluation:\n",
      "2016-12-13T07:48:58.104981: step 1200, loss 1.11386, acc 0.484\n",
      "2016-12-13T07:51:03.401921: step 1250, loss 1.14029, acc 0.386667\n",
      "2016-12-13T07:53:08.985660: step 1300, loss 1.12637, acc 0.386667\n",
      "\n",
      "Dev set Evaluation:\n",
      "2016-12-13T07:53:55.680817: step 1300, loss 1.15664, acc 0.373667\n",
      "2016-12-13T07:56:00.303524: step 1350, loss 1.1276, acc 0.333333\n",
      "2016-12-13T07:58:06.353096: step 1400, loss 1.09341, acc 0.533333\n",
      "\n",
      "Dev set Evaluation:\n",
      "2016-12-13T07:58:52.831366: step 1400, loss 1.17048, acc 0.414667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a62e378fd87b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a62e378fd87b>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch, s_batch)\u001b[0m\n\u001b[1;32m     75\u001b[0m             _, step,loss, accuracy = sess.run(\n\u001b[1;32m     76\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/envs/nlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#####\n",
    "##Start Training\n",
    "#####\n",
    "\n",
    "# Hyperparameters\n",
    "######################################################\n",
    "M = 124  #Embedding Size\n",
    "FS = [3,4,5] #Filter Sizes\n",
    "Num_F = 124 #Number of filters per filte rsize\n",
    "dropout_keep_prob = 1\n",
    "L = 100\n",
    "batch_size = 75\n",
    "num_epoch = 100\n",
    "#####################################################\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "   \n",
    "    sess = tf.Session(config=tf.ConfigProto(device_filters=\"/cpu:0\"))\n",
    "    with sess.as_default():\n",
    "        cnn = CNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size = M,\n",
    "            filter_sizes = FS,\n",
    "            num_filters = Num_F,\n",
    "            l2_reg_lambda = L)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        #timestamp = str(int(time.time()))\n",
    "        #out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        #print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        \n",
    "        # Summaries for loss and accuracy\n",
    "        #loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        #acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        #train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        #train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        #train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        #checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        #checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        #if not os.path.exists(checkpoint_dir):\n",
    "        #    os.makedirs(checkpoint_dir)\n",
    "        #saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        # Write vocabulary\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch, s_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.input_suprise: s_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step,loss, accuracy = sess.run(\n",
    "                [train_op, global_step, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if step % 50 == 0:\n",
    "              print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "        \n",
    "        def dev_step(x_batch, y_batch, s_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.input_suprise: s_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            step, loss, accuracy = sess.run(\n",
    "                [global_step, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if step % 50 == 0:\n",
    "              print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))     \n",
    "           \n",
    "        # Generate batches\n",
    "        batches = data_processor.batch_iter(\n",
    "            list(zip(x_train, y_train, surprise_train)), batch_size, num_epoch)\n",
    "        # Training loop. For each batch...\n",
    "        \n",
    "        for batch in batches:\n",
    "            x_batch, y_batch, s_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch, s_batch)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % 100 == 0:\n",
    "                print(\"\\nDev set Evaluation:\")\n",
    "                dev_step(x_dev, y_dev, surprise_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = dp.one_iter(list(zip(x_test, y_test, surprise_test))) \n",
    "for i in test:\n",
    "  x_btest, y_btest, s_btest = zip(*i)\n",
    "\n",
    "x_text = dp.load_features(test.text, doc_n)\n",
    "x_test = np.array(list(vocab_processor.transform(x_text)))\n",
    "\n",
    "#Create the numerical feature of earning surprise for our models\n",
    "surprise_test = np.array(test.Surprise, (surprise_test.shape[0], 1))\n",
    "\n",
    "y_test = create_label(test.label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

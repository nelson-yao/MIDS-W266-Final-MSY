{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words generation from the parsed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import parsing_tools as prs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34mReading 1500 zipped files ...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prs.parse_files(folder='../data/8K_gz/', output_folder='../data/parsed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Get parsed files\n",
    "csv_files = glob.glob('../data/parsed/*.csv')\n",
    "raw_files = glob.glob('../data/8K-gz/*.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START!\n"
     ]
    }
   ],
   "source": [
    "print 'START!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Variable-parsing functions\n",
    "\n",
    "META_LINES = ['FILE', 'TIME', 'EVENTS', 'ITEM']\n",
    "\n",
    "# regex templates\n",
    "reg_FILE = re.compile('FILE:(.*)')\n",
    "reg_TIME = re.compile('TIME:(.*)')\n",
    "reg_EVENTS = re.compile('EVENTS:(.*)')\n",
    "reg_ITEMS = re.compile('ITEM:(.*)')\n",
    "\n",
    "# A parsing_function generator\n",
    "def factory_parser(compiled_regex):\n",
    "    return lambda d: map(lambda x: x.replace(',', '').strip(), re.findall(compiled_regex, d))\n",
    "\n",
    "get_file = factory_parser(reg_FILE)\n",
    "get_time = factory_parser(reg_TIME)\n",
    "get_events = factory_parser(reg_EVENTS)\n",
    "get_items = factory_parser(reg_ITEMS)\n",
    "\n",
    "def get_text(doc):\n",
    "    lines = [ ln for ln in doc.split('\\n') if ln != ' ' ]\n",
    "    g_lines = [ ln for ln in lines if not any(map(lambda x: ln.startswith(x), META_LINES)) ]\n",
    "    return '\\n'.join( g_lines )\n",
    "\n",
    "# Bag of Words parser\n",
    "def process_bow(text):\n",
    "    \"\"\" Returns a Counter with the Bag of Words of the text inputted \"\"\"\n",
    "    _text = text.lower().replace('TEXT:', '').replace('Table of Contents', '')\n",
    "\n",
    "    # Split into words and remove common (NLTK) stopwords\n",
    "    words = filter(lambda x: x not in STOPWORDS, _text.split())\n",
    "\n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "## Full folder parser function\n",
    "\n",
    "def parse_files(folder, output_folder='../data/parsed/'):\n",
    "    \"\"\" Unzips all .gz files found in the specified folder \"\"\"\n",
    "\n",
    "    files = glob.glob( '{f}/*.gz'.format(f=folder) )\n",
    "\n",
    "    print( '\\n\\033[34mReading {} zipped files ...\\033[0m'.format(len(files)) )\n",
    "\n",
    "    for gzip_file in files:\n",
    "\n",
    "        with gzip.open(gzip_file, 'rb') as f_in:\n",
    "            _data = f_in.read()\n",
    "\n",
    "        \n",
    "        # Split into documents\n",
    "        documents = _data.replace('<DOCUMENT>', '').split('</DOCUMENT>')\n",
    "\n",
    "        # Parse documents and create dataframe\n",
    "        parsed_data = {\n",
    "            'file': list(map(get_file, documents)),\n",
    "            'time': list(map(get_time, documents)),\n",
    "            'events': list(map(get_events, documents)),\n",
    "            'items': list(map(get_items, documents)),\n",
    "            'text': list(map(get_text, documents))\n",
    "        }\n",
    "\n",
    "        data = pd.DataFrame.from_dict(parsed_data).iloc[:-1, :]\n",
    "\n",
    "    \n",
    "        # Generate final (parsed) features\n",
    "        data['bow'] = data.text.apply( lambda x: str(process_bow(x)) )\n",
    "        data['date'] = data.time.apply( lambda x: pd.datetime(year=int(x[0][:4]),\n",
    "                                                              month=int(x[0][4:6]),\n",
    "                                                              day=int(x[0][6:8])) )\n",
    "        data['orig_file'] = data.file.apply( lambda x: x[0] )\n",
    "\n",
    "        # replace the '.gz' with '.csv'\n",
    "        output_file = gzip_file.replace('.gz', '.csv').split('/')[-1]\n",
    "        data[['date', 'bow', 'items', 'text', 'orig_file' ]].to_csv(output_folder+output_file, index=False)\n",
    "\n",
    "        break\n",
    "\n",
    "    print( '\\033[34mDone! Located in:  \\033[32m{}\\033[0m'.format(output_folder) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34mReading 1500 zipped files ...\u001b[0m\n",
      "\u001b[34mDone! Located in:  \u001b[32m../data/parsed/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "parse_files('../data/8K_gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
